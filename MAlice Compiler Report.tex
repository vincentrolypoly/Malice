\documentclass[8pt, a4paper]{article}
\usepackage{fullpage}

\begin{document}
\title{The MAlice Compiler}
\author{Martin Donlin, Le Hoang, Eleanor Vincent}
\maketitle
\small
\section{Product}

The specification of the task was to build a full compiler; the front end, comprising of a lexical analysier, syntactic analyser and semantic analyser, and the back-end, where it will generate suitable executable code. We have elected to compile MAlice down to the x86-64 Intel Assembly Architecture using NASM syntax. 

The compiler is reliable in its execution since we manage memory ourselves and ensure that all memory we use it first initialised. This reduces the probability of unexpected behaviour. The compiler produces executable code relatively quickly since we use Bison and Flex which generate very fast code. Although there is a slight performance issue in that part of our implementation requires the use of dynamic casting which means that g++ needs to turn on its RTTI feature. As such we do have a reasonably lengthy compile time for our program, though the main length is for the main program; each individual component that requires compilation takes very little time to do so. We chose to use g++ because it is a smaller compiler than gcc. We did not need the entire collection of compilers that gcc supplies since our entire implementation is done in C++. 

The initial code generated by the code generator is considered "safe". This is because the code generator will add in push and pop instructions to save and restore registers regardless of whether or not they are useful. We decided to use a peephole optimisation in order to reduce the amount of redundant code generated by the code generator. The optimiser will search through the immediate representation of the program and find useless statements such as "add RAX, 0". Not only does this improve performance of the generated program but it also reduces the overall size of the program. By using this sort of optimisation we maintain the readability of the code so the code is easier to debug.

Given our use of visitor pattern in both ends of the compiler all of the semantic and code generation methods have been extracted into their respective class files. This means that should an outside programmer want to extend the MAlice language they would simply need to define some new node class files for their feature and then add the related tokens and methods into the lexer, parser, code generator and semantic checker. There is no need to change any of the existing code when extending the language in this manner. There are some portability drawbacks between the code generator and the semantic checker, however. As the semantic checker is built to deal with all of the error messages the code generator assumes that it will encounter no semantic errors whilst it is running. If an outside programmer where to take our implementation for code generation and link it with their own semantics, they may run different checks and so the code generation may not run correctly.

This would certainly be a program worth future development as it does have a solid code basis on which further optimisations could be made, given enough time. Our modular design means that the compiler is easy to extend and maintain. Although, since we did not have a formal specification for the MAlice language, we had to make many assumptions about the language. This resulted in a lot of edge case tests scattered throughout the compiler. The main focus of future development would be to tidy the grammar of the program to reduce code duplication and edge cases, using the same class implementation as exists now.

\section{Design Choices}

We elected to build the compiler using C++ due to the fact that allowed us to manage the memory ourselves. With this we were able to make the compiler execute faster than if we had a language such as Java doing garbage collection for us. The tools used in building this compiler were Flex, a lexer to tokenise the program and pass this output into the parser, Bison. Although Bison and Flex were originally designed for C, newer versions have a compatibility for generating C++. We also found that Bison 2.6+ has changed slightly from older Bison versions meaning that there is no certain backwards compatability. We chose to use Bison(2.4) and Flex because from our research we found that they were very popular parser generators. A potential downside to using this combination of lexer and parser generator is that it is very difficult to unlink the dependacy that Flex and Bison have on each other.

Both tools are relatively old but their basic implementation makes them sufficiently flexible for use in producing the front end compiler. Bison is also an LALR(K) parser, giving a much higher level of freedom when defining the grammer than a tool such as ANTLR, an LL(k) parser. Bison's ability to do unlimited look-aheads means it is a much more flexible tool to work with. ANTLR has the added functionality being able to generate the AST automatically, and also being able to generate a graphical representation of your AST, but its inability to use left recursion would have made defining the grammer much more difficult. Choosing to use a tool with less features made it so that we had much more control over what our parser generator generated. Tools such as ANTLR were feature rich but this meant that they we would have needed to carry around ANTLR's library with us in order to do any lexer or parser generation. This also meant that we would have less control over the kind of AST that ANTLR would have automatically generated for us.

We had complete control over the AST that Bison would generate for us because of how Bison is designed. Bison allows us to match specific commands with some combination of tokens given by the lexer . As a consequence to this, we found the AST generation relatively simple since we only had to match which AST Node would be generated by which combination of tokens. Our AST Nodes are made using an inheritance hierachy. All Nodes in the AST are derived from a single superclass. There are also smaller subclasses for grouping together AST Nodes with similar informaton such as binaryExpressionASTNode and UnaryExpressionASTNode which are both types of ExpressionASTNode which is derived from the superclass. This was required in order for us to use to visitor pattern to visit each node of the tree.  

At the time of AST generation, we also decided to use Bison and Flex's error handling features. We did not need to explicitly keep track of line numbers ourselves since Bison and Flex have a feature for that which we can enable using "\%option yylineno" and "YYERROR\_VERBOSE". This also meant we did not need to do any explicit calls to error methods. Bison has its own error method "yyerror" which is automatically called when Bison's generated file encounters a syntax error.

A depth-first search is used on the AST with the semantic checker in order to preserve the order in which the program is run. Each node had its own check function within the semantic checker file which is implemented through the use of the visitor pattern. With this all of the check functions for each of the nodes can be collected into a seperate file, both for the semantic check and later in the process for the code generation, which significantly decreases clutter within the nodes and makes code maintanence much easier. This also removes the requirement for a tree walker as the nodes will essentially 'walk themselves'. After this the AST is known to be semantically correct and, as mentioned previously, the code generator will assume that it will encounter no errors.

Once this stage is completed, the AST will be traversed again, this time with each node being converted into many Code Generation Nodes, an intermediate representation of the final code, which is then placed into a deque of other Code Generation Nodes. Each codeGenNode has a function associated with it such they are aware how they are required to print. This is a strong implementation due to the fact it eliminates the requirement to continuously read and write to file; it will only start writing to file once the code generator has finished traversing the entire AST and converting it into the intermediate representation. All of the optimisation (node rearranging and removal of unnecessary lines) can be performed on the deque itself. This makes it easier to optimise since we are working with our own custom node objects and not on strings inside a file.

We chose 64 bit architecture because it has more general purpose registers meaning we could be more flexible with our implementation. We also liked the ability to store larger values and the calling conventions are faster because we no longer need to push and pop from the stack when we invoke C functions. Instead we pass by using the general purpose registers. A noticable drawback with this however is that our compiler is not compatible with 32 bit architectured machines. Also, using 64 bit architecture means that we needed to be more careful with how we read and write to memory since the assembler will not always write 64 bits into a memory address so we can have unexpected results when two 32 bit instructions get written into a 64 bit memory location unexpectedly.

Luckily, a lot of the assumptions we made whilst making the front end compiler translated well over to the back end compiler. Most of our assumptions were correct and we found that we were required to make very little changes to the front end compiler before starting the back end. This was purely by chance and if we were to do this project again, we would like to be able to make a full compiler for a smaller language set. With all the basic functionality in place, we would not need to assume as many things as before and we would just expand our currently compiler implementation with new features by adding additional layers onto the already functional compiler.

One significant issue during the build was the failure to keep code seperated into suitable header files as progress was made. Initially all of the nodes were compiled in one large header file but it eventually became evident that this file was cumbersome to navigate through. A large amount of time was lost whilst fixing this situation as extracting individual classes that have dependacies on their base classes is difficult to do since we need to link them together. The main issue being how to correctly avoid circular dependacies between files. Progress speed did increase once this was completed and header files were kept suitably seperate from this point on. Were we to repeat this project we would ensure we did not fall into the same pit-trap again.

\section{Beyond the Specification}

Currently we have focused on language extensions for MAlice, having chosen so far to implement constant variables, VBA styled for-loops and the ability to determine the length of an array. Constant variables was a simple initial test to see how easy language extension was within our compiler, and the results were remarkably quick and easy. The tokens and grammar rules were added into the lexer and the parser and then it became a simple task of adding a check method into the semantic checker file and the code generation file. There was no need to modifying any of the existing code, which proves how the maintanence of the compiler could be so easy. After this we elected to focus on language extensions relating to traversing arrays; knowing the size of an array is an important piece of information when traversing arrays whilst for loops are a good mechanism for this. To be able to retrieve the size of the array at any given time, we stored the array as an object with one extra space at the top. Inside this space we put the size of the array calculated at run-time. This meant we had to change how arrays were indexed because now they are offset by 1. We also chose to expand the language by making MAlice sound more like a natural language. We changed the lexer and parser to allow more than one way of invoking commands. For example, if you wanted to add two numbers you could say "x + y" or "x added to y" or "x plus y". This made the language more natural to read since the previous langauge had some english sentences followed by symbols scatter everywhere. Other useful features that we might look into given more time are more string manipulations and object-orientated features such as implementating classes into MAlice. 

Another feature we added was that we linked the compiler with gcc and C standard functions and libraries. This meant that during code generation, we can call any of the C standard functions so long as we save and restore registers appropriately and that we pass the correct arguments onto the function. This also means we can read and write to stdout/stdin without having to worry about sending interupts or storing buffers.  

We also added a more efficient way to use registers. We found that we could reduce the maximum number of registers used by simply reversing the order in which binary expressions were calculated. We implemented a "weight" function for each ExpressionASTNode that would return the weight of itself which is dependant on how many registers it uses. In order to use the least number of registers, we needed to do the highest weighted branch first. As mentioned above, we also decided to do some peephole optimisations to reduced redundant code. This was a simple matter of adding in a "removable" function into each codeGenNode so that each node can check itself on whether or not it is useless. If it is then it will remove itself from the deque of nodes. The drawback with the peephole optimisation is that each node can only see itself. So if a list of actions proved redundant, then the nodes would not know because individually they are not. For example, MOV RAX, RBX , MOV RBX, RAX would be a redundant list of commands. However each command individually is not useless. In order to solve this we needed to implement some sort of scanner that would scan the deque for these redundant patterns. The drawback to this is that there are potentially infinitely many cases that could lead to a redundant list of commands. 

As a language, MAlice is still very limited in what it can actually do and implementing classes would go some way in fixing this issue. Currently there is only one storage container for arrays and defining classes may enable there to be multiple storage containers for them. Defining classes would simply involve defining the appropriate tokens and grammar rules within the lexer and parser before moving on to define suitable check functions for the semantic checker and code generator. Then we wrap the programNode file within a class so that multiple programNodes could be define, each holding it's own class. Then within each programNode there would be pointers to each variable, declaration, loop and function contained within each classes; the rest of the code would work as normal.

Alongside this there are possibilities for implementating dynamic memory allocation, garbage collection and adding automated error recovery into the compiler itself. Currently the compiler, on throwing an error, will simply abandon the whole process even if the error is a simple syntactical one such as neglecting a full stop at the end of a statement. It might be interesting to see if it were possible to code the compiler to recognise simple syntatical errors like this and for it to fix the file for the user and continue on with compilation. It would be best if, in the case it did fix the code like this, that the compiler returned a message on completion of where it had made these insertions, in the possible event that it misjudges the meaning behind the code. 

\end{document}
